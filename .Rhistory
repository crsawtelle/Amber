trumpsdoom_emotions$book <- "The Trumps of Doom"
bloodamber_emotions <- blood_amber$text %>%
get_sentences() %>%
emotion_by(drop.unused.emotions = TRUE) %>%
group_by(emotion_type) %>%
summarise(ave_emotion = mean(ave_emotion)) %>%
mutate(emotion_type = reorder(emotion_type, ave_emotion))
bloodamber_emotions$book <- "Blood of Amber"
signchaos_emotions <- sign_chaos$text %>%
get_sentences() %>%
emotion_by(drop.unused.emotions = TRUE) %>%
group_by(emotion_type) %>%
summarise(ave_emotion = mean(ave_emotion)) %>%
mutate(emotion_type = reorder(emotion_type, ave_emotion))
signchaos_emotions$book <- "Sign of Chaos"
knightshadows_emotions <- knight_shadows$text %>%
get_sentences() %>%
emotion_by(drop.unused.emotions = TRUE) %>%
group_by(emotion_type) %>%
summarise(ave_emotion = mean(ave_emotion)) %>%
mutate(emotion_type = reorder(emotion_type, ave_emotion))
knightshadows_emotions$book <- "Knight of Shadows"
princechaos_emotions <- prince_chaos$text %>%
get_sentences() %>%
emotion_by(drop.unused.emotions = TRUE) %>%
group_by(emotion_type) %>%
summarise(ave_emotion = mean(ave_emotion)) %>%
mutate(emotion_type = reorder(emotion_type, ave_emotion))
princechaos_emotions$book <- "Prince of Chaos"
emotions_bind <- rbind(amber_emotions, nineprinces_emotions, gunsavalon_emotions, signunicorn_emotions, handoberon_emotions, courtschaos_emotions, trumpsdoom_emotions, bloodamber_emotions, signchaos_emotions, knightshadows_emotions, princechaos_emotions)
mycolors <- c("black", "firebrick","orangered3", "salmon3", "gold4", "green4", "blue", "navy", "purple4", "darkviolet", "deeppink4", "pink4")
emotions_bind %>%
mutate(book = fct_relevel(book, "Chronicles of Amber", "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "The Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
ggplot(aes(reorder(emotion_type, -ave_emotion), ave_emotion, fill = book)) +
geom_col(position = "dodge") +
scale_fill_manual(values = mycolors) +
ggthemes::theme_gdocs() +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10)) +
scale_x_discrete(guide = guide_axis(angle = 45)) +
labs(title = "Emotion Classification of Text by Book", x = "Emotion Type", y = "Average Emotion of Text",
caption = str_wrap("Graph 3: Displaying emotion count by each line of text, returning a count for each emotion it detects (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, and any negation to those emotions)."))
# Get most used words, including stop words
book_words <- chronicles_of_amber() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
head(book_words)
# Zipf's law states that the frequency that a word appears is inversely proportional to its rank
# - rank tells us the rank of each word within the frequency table - How important is it
freq_by_rank <- book_words %>%
group_by(book) %>%
mutate(rank = row_number(), term_freq = n/total)
tail(freq_by_rank)
# Calculating the tf-idf (term frequency - inverse document frequency) attempts to find the words that are important (common) in the text, but not too common
# when idf and tf-idf are zero, these are extremely common words. This approach decreases the weight for those common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.
# tf-idf will allow us to find words that are characteristic for one book within all the books. Words that are more common in one book than another.
book_words <- book_words %>%
bind_tf_idf(word, book, n)
head(book_words)
# look at terms with higher tf-idf - typically proper nouns (i.e. names)
head(book_words %>%
select(-total) %>%
arrange(desc(tf_idf)))
book_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = reorder_within(word, tf_idf, book)) %>%
group_by(book) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(tf_idf, word, fill = factor(book))) +
geom_col(show.legend = FALSE) +
labs(title = "Unigram TF-IDF Words from the Chronicles of Amber", x = "TF-IDF of Words", y = "Words",
caption = "Graph 3: Displaying the top five TF-IDF words for each book.") +
facet_wrap(~book, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 12, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
scale_y_reordered()
# N-Gram analysis
# bigram most frequent words that appear next to each other - token = "ngrams", n = 2
# create bigrams of the chronicles of amber
amber_grams <- amber_books %>%
filter(chapter > 0) %>%
unnest_tokens(bigram, text, token = "ngrams", n=2)
# view the most used bigrams
# amber_grams %>%
#   count(bigram, sort = TRUE)
snowball_sw <- get_stopwords("en", "snowball")
# adding tir na nog'th to stop words
word = c("tir", "na", "nog'th", "said")
lexicon = c( "custom",  "custom", "custom", "custom")
data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, snowball_sw)
# split the bigrams into two separate columns word1 and word2
bigrams_separated <- amber_grams %>%
separate(bigram, c("word1", "word2"), sep = " ")
# remove stopwords from bigrams
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% custom_stop_words$word) %>%
filter(!word2 %in% custom_stop_words$word) %>%
# filter(!word3 %in% stop_words$word) %>%
filter(!is.na(word1)) %>%
filter(!is.na(word2))
# filter(!is.na(word3))
# new bigram counts without stopwords
bigram_counts <- bigrams_filtered %>%
filter(chapter > 0) %>%
count(word1, word2, sort = TRUE)
head(bigram_counts)
# Process tf-idf for bigrams
bigram_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
bigram_united
bigram_tf_idf <- bigram_united %>%
group_by(book) %>%
count(book, bigram) %>%
bind_tf_idf(bigram, book, n) %>%
slice_max(tf_idf, n = 5) %>%
ungroup() %>%
arrange(desc(tf_idf))
bigram_tf_idf
bigram_tf_idf %>%
mutate(bigrams = reorder_within(bigram, tf_idf, book)) %>%
ggplot(aes(tf_idf, bigrams, fill = factor(book))) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 12, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Bigrams TF-IDF for the Chronicles of Amber", y = "Bigram", x = "tf-idf of Bigram",
caption = "Graph 4: Displaying the top five tf-idf bigrams for each book.") +
scale_y_reordered()
left_right_bigram <- bigrams_filtered %>%
filter(word1 == c("right", "left")) %>%
count(word1, word2, sort = TRUE) %>%
mutate(word2 = reorder(word2, n)) %>%
group_by(word1) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(word2, n, fill = word1)) +
geom_col(show.legend = FALSE) +
facet_wrap(~word1, ncol = 2, scales = "free_y") +
scale_fill_manual(values = c("turquoise4", "deeppink4")) +
ggthemes::theme_gdocs() +
labs(title = "Top 10 Bigrams for Left and Right") +
coord_flip()
left_right_bigram
# right_bigrams <- bigrams_filtered %>%
#   filter(word1 == "right") %>%
#   count(word1, word2, sort = TRUE)
# Using afinn lexicon (giving a positive/negative score to each word) to graph words that appear after the word not
AFINN <- get_sentiments("afinn")
not_word <- bigrams_separated %>%
filter(word1 == "not") %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word2, value, sort = TRUE) %>%
ungroup()
not_word
# Graph of words
not_word %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n*value, fill = n * value > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by \"not\"") +
ylab("Afinn sentiment score * number of occurences") +
coord_flip()
# Network of bigrams graph - use igraph package
bigram_network_graph <- bigram_counts %>%
filter(n > 10) %>%
graph_from_data_frame()
# bigram_network_graph
# graph network of bigrams - use ggraph package
# Used to show clusters of words that appear next to each other
set.seed(486)
a <- grid::arrow(type = "closed", length = unit(.07, "inches"))
ggraph(bigram_network_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.05, "inches")) +
geom_node_point(color = "navy") +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
labs(title = "Network of Bigrams")
theme_void()
# Counting and correlating pairs of words - use widyr package
# - The most often used pair of words within a 10 row section is "amber" and "time", however, time and amber are one of the most used words in the novel.
# adding tir na nog'th to stop words
word = c("tir", "na", "nog'th")
lexicon = c( "custom",  "custom", "custom")
data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, snowball_sw)
# separate the books and see which words tend to fall into the same sections - every 10 lines
amber_section_words <- chronicles_of_amber() %>%
mutate(section = row_number() %/% 10) %>% # separate the books into sections of 10 rows
filter(section > 0) %>%
unnest_tokens(word, text) %>%
filter(!word %in% custom_stop_words$word)
# count the words co-occurring within each section - create word pairs using pairwise_count function
word_pairs <- amber_section_words %>%
pairwise_count(word, section, sort = TRUE)
head(word_pairs %>%
filter(item1 == "time"), 10)
head(word_pairs %>%
filter(item1 == "amber"), 10)
word_cor <- amber_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
head(word_cor)
mycolors2 <- c("turquoise4", "navy", "purple4", "deeppink4")
word_cor %>%
filter(item1 %in% c("pattern", "amber", "logrus", "shadow")) %>%
group_by(item1) %>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder_within(item2, correlation, item1)) %>%
ggplot(aes(correlation, item2, fill = item1)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~item1, scales = "free_y") +
scale_fill_manual(values = mycolors2) +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Top 10 Correlated Words", x = "Word", y = "Correlation",
caption = "Graph 5: Displaying the top 10 words correlated with 'Amber', 'Logrus', 'Pattern', and 'Shadow'.") +
scale_y_reordered()
set.seed(486)
word_cor %>%
filter(correlation > 0.21) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "violet", size = 2) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
nine_princes_in_amber <- amber_tidy %>%
group_by(book) %>%
filter(book == "Nine Princes in Amber")
nine_princes_in_amber
afinn <- nine_princes_in_amber %>%
inner_join(get_sentiments("afinn"), by = join_by(word)) %>%
group_by(index = linenumber %/% 40) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(
nine_princes_in_amber %>%
inner_join(get_sentiments("bing"), by = join_by(word)) %>%
mutate(method = "BING"),
nine_princes_in_amber %>%
inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
filter(sentiment %in% c("positive", "negative")) %>%
mutate(method = "NRC")
)
bing_and_nrc <- bing_and_nrc %>%
count(method, index = linenumber %/% 40, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn, bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method )) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
# Finding the most used words based on the nrc lexicon for just the book Nine Princes in Amber
bing_word_counts <- nine_princes_in_amber %>%
inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
coord_flip()
# Topic modeling - per-topic-per-word probabilities, $\beta$
# Identifying top word used in each chapter of each book
amber_by_chapter <- amber_books %>%
group_by(book) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, book, chapter)
amber_by_chapter_word <- amber_by_chapter %>%
unnest_tokens(word, text)
word_counts <- amber_by_chapter_word %>%
anti_join(stop_words, by = join_by(word)) %>%
count(document, word, sort = TRUE)
head(word_counts, 10)
# Cast tidy data into a document text matrix
# Sparsity - sparse matrix is a matrix where most of the elements are zero. When working with text data, we say our data is "sparse" because most documents do not contain most words, resulting in a representation of mostly zeros. There are special data structures and algorithms for dealing with sparse data that can take advantage of their structure. For example, an array can more efficiently store the locations and values of only the non-zero elements instead of all elements. - Supervised Machine Learning for Text Analysis in R Emil Hvitfeldt and Julia Silge
amber_chapter_dtm <- word_counts %>%
cast_dtm(document, word, n)
amber_chapter_dtm
# Latent Dirichlet Allocation (LDA) - treats each document as a mixture of topics, and each topic as a mixture of words
# Break down document term matrix into 10 different topics, iterating 500 times. With only one iteration, the topics where duplicated. For example, the word "time" was the top term for 5 out of the 10 topics.
# "Gibbs sampling is a method of Markov chain Monte Carlo (MCMC) that approximates intractable joint distribution by consecutively sampling from conditional distributions."
amber_chapter_lda <- LDA(amber_chapter_dtm, k = 10, method="Gibbs", control=list(seed = 486, iter = 500, verbose = 25))
terms(amber_chapter_lda, 5)
topics(amber_chapter_lda)
# Word topic probability - $\beta$ the model computes the probability of that term being generated from that topic
# For example, random has a .00000997 probability of appearing in topic 1, but a 0.01794711 probability of appearing in topic 4, etc.
amber_chapter_topic <- tidy(amber_chapter_lda, matrix = "beta")
head(amber_chapter_topic)
# Top five terms probability per topic - ganelon has the highest probability, 0.029, of showing up in topic 1
# showing the top 5 terms prob per topic
amber_top_terms <- amber_chapter_topic %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
ungroup() %>%
arrange(topic, -beta)
head(amber_top_terms, 10)
mean(amber_top_terms$beta)
# Visual representation of top five word probability of showing up with each topic.
amber_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
labs(title = "Top 5 Word Probability (Beta) Per Topic", x = "Beta Probability", y = "Term",
caption = "Graph 6: Visual representation of top five word with the highest beta probability of showing up in each topic.") +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
scale_y_reordered()
# Visualizing - for topic 1, any gamma greater than 0.2. We can see that approximately 100% of the words from Sign of Chaos Chapter 8, 10, 11, and 12 are generated from topic 1. However, we can also see that 100% of the words in Knight of Shadows chapter 6, 7, and 12 are also generated from topic 1.
amber_chapter_gamma <- tidy(amber_chapter_lda, matrix = "gamma")
# head(amber_chapter_gamma)
amber_gamma_graph <- amber_chapter_gamma %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(topic, -gamma)
# head(amber_top_terms, 10)
amber_gamma_graph %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Top 5 Document Probability (Gamma) per Topic", x = "Gamma Probability", y = "Document",
caption = "Graph 7: Visual representation of top five documents with the highest gamma probability of showing up in each topic." ) +
scale_y_reordered()
# median(amber_chapter_gamma$gamma)
# Split book and chapter to view per-document-per-topic probability
amber_chapter_gamma <- amber_chapter_gamma %>%
separate(document, c("title","chapter"), sep = "_", convert = TRUE)
head(amber_chapter_gamma)
# Each chapter is supposed to be uniquely assigned to a specific topic.
amber_chapter_gamma %>%
mutate(title = fct_relevel(title, "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "The Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
ggplot(aes(factor(topic), gamma, fill = title)) +
geom_boxplot(show.legend = FALSE) +
facet_wrap(~title, ncol = 2) +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Gamma Probability Assignment by Book/Topic" , x = "topic", y = expression(gamma),
caption = "Graph 8: Displaying Topic modeling - per-document-per-topic probabilities, gamma.")
# Classification of the topics - topic 1 has a gamma probability of approximately 25% chance of appearing in The Guns of Avalon chapter 1
chapter_classification <- amber_chapter_gamma %>%
group_by(title, chapter) %>%
top_n(1, gamma) %>%
ungroup()
head(chapter_classification, 10)
# 284 rows out of 350 rows or 81% of the topics were misclassified. This makes sense because of the series has the same theme/topics throughout the series and a lot of the same characters.
book_topics <- chapter_classification %>%
count(title, topic) %>%
group_by(title) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = title, topic)
chpt_consensus <- chapter_classification %>%
inner_join(book_topics, by = "topic") %>%
filter(title != consensus)
head(chpt_consensus)
# augmenting - assigning each word in each document to a topic - the more words in a document assigned to that topic, generally, the more weight gamma will go on that document-topic classification
# returns a tidy data frame of book-term counts
# Because the LDA algorithm is stochastic, it can land on a topic that spans multiple books (stochastic = randomly determined; having random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely)
assignments <- augment(amber_chapter_lda, data = amber_chapter_dtm)
head(assignments, 10)
assignments <- assignments %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
inner_join(book_topics, by = c(".topic" = "topic"))
head(assignments)
assignments %>%
count(title, consensus, wt=count) %>%
mutate(across(c(title, consensus), ~str_wrap(., 20))) %>%
group_by(title) %>%
mutate(percent = n/sum(n)) %>%
ggplot(aes(consensus, title, fill = percent)) +
geom_tile() +
scale_fill_gradient2(high = "navy", label = percent_format()) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1), panel.grid = element_blank()) +
labs(x = "Book words were assigned to",
y = "Book words came from",
fill = "% of assignment")
# what were the most commonly mistaken words
wrong_words <- assignments %>%
filter(title != consensus)
wrong_words %>%
count(title, consensus, term, wt = count) %>%
ungroup() %>%
arrange(desc(n))
# Latent Dirichlet Allocation (LDA) - treats each document as a mixture of topics, and each topic as a mixture of words
# Break down document term matrix into 10 different topics, iterating 500 times. With only one iteration, the topics where duplicated. For example, the word "time" was the top term for 5 out of the 10 topics.
# "Gibbs sampling is a method of Markov chain Monte Carlo (MCMC) that approximates intractable joint distribution by consecutively sampling from conditional distributions."
amber_chapter_lda <- LDA(amber_chapter_dtm, k = 10, method="Gibbs", control=list(seed = 486, iter = 500, verbose = 25))
terms(amber_chapter_lda, 5)
topics(amber_chapter_lda)
# Word topic probability - $\beta$ the model computes the probability of that term being generated from that topic
# For example, random has a .00000997 probability of appearing in topic 1, but a 0.01794711 probability of appearing in topic 4, etc.
amber_chapter_topic <- tidy(amber_chapter_lda, matrix = "beta")
head(amber_chapter_topic)
# Word topic probability - $\beta$ the model computes the probability of that term being generated from that topic
# For example, random has a .00000997 probability of appearing in topic 1, but a 0.01794711 probability of appearing in topic 4, etc.
amber_chapter_topic <- tidy(amber_chapter_lda, matrix = "beta")
head(amber_chapter_topic)
# Top five terms probability per topic - ganelon has the highest probability, 0.029, of showing up in topic 1
# showing the top 5 terms prob per topic
amber_top_terms <- amber_chapter_topic %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
ungroup() %>%
arrange(topic, -beta)
head(amber_top_terms, 10)
mean(amber_top_terms$beta)
# Visual representation of top five word probability of showing up with each topic.
amber_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
labs(title = "Top 5 Word Probability (Beta) Per Topic", x = "Beta Probability", y = "Term",
caption = "Graph 6: Visual representation of top five word with the highest beta probability of showing up in each topic.") +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
scale_y_reordered()
# Visualizing - for topic 1, any gamma greater than 0.2. We can see that approximately 100% of the words from Sign of Chaos Chapter 8, 10, 11, and 12 are generated from topic 1. However, we can also see that 100% of the words in Knight of Shadows chapter 6, 7, and 12 are also generated from topic 1.
amber_chapter_gamma <- tidy(amber_chapter_lda, matrix = "gamma")
# head(amber_chapter_gamma)
amber_gamma_graph <- amber_chapter_gamma %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(topic, -gamma)
# head(amber_top_terms, 10)
amber_gamma_graph %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, ncol = 2, scales = "free_y") +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Top 5 Document Probability (Gamma) per Topic", x = "Gamma Probability", y = "Document",
caption = "Graph 7: Visual representation of top five documents with the highest gamma probability of showing up in each topic." ) +
scale_y_reordered()
# median(amber_chapter_gamma$gamma)
# Split book and chapter to view per-document-per-topic probability
amber_chapter_gamma <- amber_chapter_gamma %>%
separate(document, c("title","chapter"), sep = "_", convert = TRUE)
head(amber_chapter_gamma)
# Each chapter is supposed to be uniquely assigned to a specific topic.
amber_chapter_gamma %>%
mutate(title = fct_relevel(title, "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "The Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
ggplot(aes(factor(topic), gamma, fill = title)) +
geom_boxplot(show.legend = FALSE) +
facet_wrap(~title, ncol = 2) +
scale_fill_manual(values = mycolors_density) +
theme(axis.text = element_text(size = 10, face = "bold"),
axis.title = element_text(size = 14, face = "bold"),
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
plot.caption = element_text(hjust = 1, size = 10),
strip.text = element_text(face = "bold", size = 12)) +
labs(title = "Gamma Probability Assignment by Book/Topic" , x = "topic", y = expression(gamma),
caption = "Graph 8: Displaying Topic modeling - per-document-per-topic probabilities, gamma.")
