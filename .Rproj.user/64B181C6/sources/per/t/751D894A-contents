---
title: "Chronicles_of_Amber"
author: "Crystal Sawtelle"
date: "STA-486C"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(Amber)
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
# tidytext is used for unnesting into tokens and cleaning data
if (!require(tidytext)) install.packages("tidytext", repos = "http://cran.us.r-project.org")
# textdata need for afinn and nrc lexicon
if (!require(tidydata)) install.packages("textdata", repos = "http://cran.us.r-project.org")
if (!require(tidyclean)) install.packages("textclean", repos = "http://cran.us.r-project.org")
# tm need for tidying document term matrix objects
if (!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
# wordcloud used for word cloud
if (!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
# reshape2 is used to make a comparison cloud
if (!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
# igraph used to manipulate and analyze networks - create columns of "from" and "to" to visualize a network
if (!require(igraph)) install.packages("igraph", repos = "http://cran.us.r-project.org")
# ggraph used to graph network created from igraph package
if (!require(ggraph)) install.packages("ggraph", repos = "http://cran.us.r-project.org")
# widyr used for counting and correlating pairs of words within sections of text
if (!require(widyr)) install.packages("widyr", repos = "http://cran.us.r-project.org")
# syuzhet used to get_sentiment for sentiment function
if (!require(syuzhet)) install.packages("syuzhet", repos = "http://cran.us.r-project.org")
# topicmodels used for LDA function - creates a n-topic LDA model
if (!require(topicmodels)) install.packages("topicmodels", repos = "http://cran.us.r-project.org")
# sentimentr detects sentiment by each line in the text
if (!require(sentimentr)) install.packages("sentimentr", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(tidytext)
library(textdata)
library(textclean)
library(tm)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(syuzhet)
library(topicmodels)
library(sentimentr)
# scales used to make a confusion matrix for misclassified words
library(scales)
# used to get readability statistics - not sure if I am using
library(quanteda)
```



```{r, echo=FALSE, include=FALSE}
# Create amber_books mutate line number and chapter

amber_books <- chronicles_of_amber() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]")))) %>%
  ungroup()

# amber_books
```


```{r, echo=FALSE, include=FALSE}
# Break out each book with line number and chapter

nine_princes <- amber_books %>%
  group_by(book) %>%
  filter(book == "Nine Princes in Amber")

guns_avalon <- amber_books %>%
  group_by(book) %>%
  filter(book == "The Guns of Avalon")
# the_guns_of_avalon

sign_unicorn <- amber_books %>%
  group_by(book) %>%
  filter(book == "Sign of the Unicorn")
# sign_of_the_unicorn

hand_oberon <- amber_books %>%
  group_by(book) %>%
  filter(book == "The Hand of Oberon")
# the_hand_of_oberon

courts_chaos <- amber_books %>%
  group_by(book) %>%
  filter(book == "The Courts of Chaos")
# the_courts_of_chaos

trumps_doom <- amber_books %>%
  group_by(book) %>%
  filter(book == "The Trumps of Doom")
# the_trumps_of_doom

blood_amber <- amber_books %>%
  group_by(book) %>%
  filter(book == "Blood of Amber")
# blood_of_amber

sign_chaos <- amber_books %>%
  group_by(book) %>%
  filter(book == "Sign of Chaos")
# sign_of_chaos

knight_shadows <- amber_books %>%
  group_by(book) %>%
  filter(book == "Knight of Shadows")
# knight_of_shadows

prince_chaos <- amber_books %>%
  group_by(book) %>%
  filter(book == "Prince of Chaos")
# prince_of_chaos
```

```{r, echo=FALSE, include=FALSE}
# using tidytext to unnest each word in the book

amber_tidy <- amber_books %>%
  unnest_tokens(word, text)
#amber_tidy
```

```{r, echo=FALSE, include=FALSE}
# remove stop words "the", "a", "if", etc.

data("stop_words")

amber_tidy <- amber_tidy %>%
  anti_join(stop_words, by = "word")

amber_tidy %>%
  count(word, sort = TRUE)

```


```{r, echo=FALSE, fig.height=8, message=FALSE, warning=FALSE, include=FALSE}
# Word frequency of entire series - top words above 600 frequency
mycolors_density <- c("red4", "firebrick","orangered3", "gold4", "green4", "turquoise4", "navy", "purple4", "darkviolet", "deeppink4" )

amber_tidy_graph <- amber_tidy %>%
  group_by(book) %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 5) %>%
  ungroup() %>%
  arrange(book, -n)

amber_tidy_graph %>%
  mutate(word = reorder_within(word, n, book)) %>%
  ggplot(aes(n, word, fill = factor(book))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = mycolors_density) +
  ggthemes::theme_gdocs() +
  labs(title = "Top 10 Used Words of the Chronicles of Amber", x = "Word Frequency") +
  scale_y_reordered()
```

**Graph 1:** Displaying the top 5 used word in each book in the Chronicles of Amber.


```{r, echo=FALSE, include=FALSE}
amber_tidy %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

\newpage

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, include=FALSE}
amber_bing_word_counts <- amber_tidy %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

amber_bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free_y") +
  labs(title = "Top 10 Negative and Positive Words in The Chronicles of Amber") +
  scale_fill_manual(values = c("turquoise4", "deeppink4")) +
  ggthemes::theme_gdocs() +
  coord_flip()
```
**Graph 3:** Identifying the top 10 positive and top 10 negative words in the book series. Will remove words that are proper nouns because they do not have a sentiment (i.e. corwin, random, amber, and luke). The **negative** graph includes the word *"chaos"*, which is misclassified because it is a place, *The Courts of Chaos*. The **positive** graph has also misclassified the word *"trump"*, it is in reference to a *trump card*, which they shorten in the series to just trump.

\newpage

```{r,fig.height=7, warning=FALSE, include=FALSE}
# Create a word cloud separated by positive and negative words.

amber_tidy %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("steelblue4", "magenta4"), max.words = 100)
  
```


```{r, echo=FALSE, include=FALSE}
# Split by chapter and find the most negative and positive chapters
amber_chapters <- chronicles_of_amber() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|chapter [\\divxlc]") %>%
  ungroup()

amber_chapters %>%
  group_by(book) %>%
  summarise(chapters = n())
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Most Negative chapter using bing lexicon (negative sentiment) - Ratio equals negative word count divided by total word count in chapter.

amber_negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

amber_word_count <- amber_tidy %>%
  group_by(book, chapter) %>%
  summarise(words = n())

amber_tidy %>%
  semi_join(amber_negative) %>%
  group_by(book, chapter) %>%
  summarize(negative_words = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = negative_words/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()

```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Most negative chapters using nrc lexicon (negative sentiment) - Ratio equals negative word count divided by total word count in chapter.

amber_negative <- get_sentiments("nrc") %>%
  filter(sentiment == "negative")

amber_word_count <- amber_tidy %>%
  group_by(book, chapter) %>%
  summarise(words = n())

amber_tidy %>%
  semi_join(amber_negative) %>%
  group_by(book, chapter) %>%
  summarize(negative_words = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = negative_words/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Get most positive chapters using bing lexicon (postive sentiment) - Ratio equals positive word count divided by total word count in chapter.

amber_positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

amber_tidy %>%
  semi_join(amber_positive) %>%
  group_by(book, chapter) %>%
  summarize(positive_words = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = positive_words/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Get the most positive chapters using nrc lexicom (postive sentiment) - Ratio equals positive word count divided by total word count in chapter.

amber_positive <- get_sentiments("nrc") %>%
  filter(sentiment == "positive")

amber_tidy %>%
  semi_join(amber_positive) %>%
  group_by(book, chapter) %>%
  summarize(positive_words = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = positive_words/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

\newpage

```{r, warning=FALSE, echo=FALSE, fig.height=8}
# graph the sentiment of every 10 lines - integer division
amber_sentiment <- amber_tidy %>%
  inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
  count(book, index = linenumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE, linewidth = 3) +
  geom_smooth(show.legend = FALSE, linewidth = 2, colour = "black", method = "loess") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  scale_y_continuous(limits = c(-5,5)) +
  scale_x_reverse() +
  labs(title = "Sentiment Patterns of Chronicles of Amber",
       caption = str_wrap("Graph 1: Graphing using nrc lexicon, filtering only words associated to the sentiment of positive and negative. Displays the difference between the number of positive and negative words found in every 10 lines of text.")) +
  scale_fill_manual(values = c("red4", "firebrick","orangered3", "gold4", "green4", "turquoise4", "navy", "purple4", "darkviolet", "deeppink4" )) +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  coord_flip()
  
```

**Graph 4:** Graphing using "nrc" lexicon, filtering only words associated to the sentiment of positive and negative. Displays the difference between the number of positive and negative words found in every 10 lines of text.  

\newpage


\newpage

```{r, warning=FALSE, echo=FALSE, include=FALSE}
# sentiment_by command to get an aggregate sentiment measure for the entire series
# - finds the sentiment by line in text and displays its denisity.

# sentiment2 <- sentiment(amber_books$text)
# summary(sentiment2$sentiment)
nineprinces_senti <- sentiment(nine_princes$text)
nineprinces_senti$book <- "Nine Princes in Amber"
gunsavalon_senti <- sentiment(guns_avalon$text)
gunsavalon_senti$book <- "The Guns of Avalon"
signunicorn_senti <- sentiment(sign_unicorn$text)
signunicorn_senti$book <- "Sign of the Unicorn"
handoberon_senti <- sentiment(hand_oberon$text)
handoberon_senti$book <- "The Hand of Oberon"
courtschaos_senti <- sentiment(courts_chaos$text)
courtschaos_senti$book <- "The Courts of Chaos"
trumpsdoom_senti <- sentiment(trumps_doom$text)
trumpsdoom_senti$book <- "Trumps of Doom"
bloodamber_senti <- sentiment(blood_amber$text)
bloodamber_senti$book <- "Blood of Amber"
signchaos_senti <- sentiment(sign_chaos$text)
signchaos_senti$book <- "Sign of Chaos"
knightshadows_senti <- sentiment(knight_shadows$text)
knightshadows_senti$book <- "Knight of Shadows"
princechaos_senti <- sentiment(prince_chaos$text)
princechaos_senti$book <- "Prince of Chaos"

books <- rbind(nineprinces_senti, gunsavalon_senti, signunicorn_senti, handoberon_senti, courtschaos_senti, trumpsdoom_senti, bloodamber_senti, signchaos_senti, knightshadows_senti, princechaos_senti)

books$book <- as.factor(books$book)
summary(books$sentiment)
```

```{r, warning=FALSE, echo=FALSE, fig.width=12}
mycolors_density <- c("red4", "firebrick","orangered3", "gold4", "green4", "turquoise4", "navy", "purple4", "darkviolet", "deeppink4" )

books %>%
  mutate(book = fct_relevel(book, "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
  filter(sentiment != 0) %>%
  ggplot(aes(sentiment, color = book)) +
  scale_color_manual(values = mycolors_density) +
  labs(title = "Sentiment Density by Book",
       caption = "Graph 2: Displays the density of the sentiment of each book excluding when sentiment equals zero. ") +
  geom_density() +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10)) +
  geom_vline(aes(xintercept=mean(sentiment)), color="navy", linetype="dashed", size=.5) +
  geom_text(aes(x=.4, y=.03, label=paste("Mean =", round(mean(sentiment), 6))), color = "navy")
```

**Graph 5:** Displays the density of the sentiment of each book excluding when sentiment equals zero. 

\newpage

```{r, warning=FALSE, echo=FALSE, fig.width=12}
books %>%
  mutate(book = fct_relevel(book, "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
  ggplot(aes(sentiment, color = book)) +
  scale_color_manual(values = mycolors_density) +
  labs(title = "Sentiment Density by Book") +
  geom_density(show.legend = FALSE) +
  geom_vline(aes(xintercept=mean(sentiment)), color="navy", linetype="dashed", size=.5) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

```{r, echo=FALSE, include=FALSE}
plotEmotion <- function(rawText){
    textSummary <- rawText %>%
    get_sentences() %>%
    emotion_by(drop.unused.emotions = FALSE) %>%
    group_by(emotion_type) %>%
    summarise(ave_emotion = mean(ave_emotion))
    
    par(mar = c(11,4,4,4))
    barplot(textSummary$ave_emotion, names.arg = textSummary$emotion_type, las=2, col="navy")
  return(textSummary)
    
}

plotEmotion(amber_books$text)
```


```{r, echo=FALSE, include=FALSE}
AB_textSummary <- amber_books$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion)) %>%
  ggplot(aes(emotion_type, ave_emotion)) +
  geom_col() +
  coord_flip() +
  labs(title = "Emotion Classification of Text", x = "Emotion Type", y = "Average Emotion of Text")
AB_textSummary
```

\newpage

```{r, echo=FALSE, include=FALSE}
# emotion_by will scan each line, count the words in the line and return an emotion count for each emotion it detects (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. It will also identify any negation to those emotions.)

amber_emotions <- amber_books$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion)) 
amber_emotions$book <- "Chronicles of Amber"

nineprinces_emotions <- nine_princes$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
nineprinces_emotions$book <- "Nine Princes in Amber"

gunsavalon_emotions <- guns_avalon$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
gunsavalon_emotions$book <- "The Guns of Avalon"

signunicorn_emotions <- sign_unicorn$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
signunicorn_emotions$book <- "Sign of the Unicorn"

handoberon_emotions <- hand_oberon$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
handoberon_emotions$book <- "The Hand of Oberon"

courtschaos_emotions <- courts_chaos$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
courtschaos_emotions$book <- "The Courts of Chaos"

trumpsdoom_emotions <- trumps_doom$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
trumpsdoom_emotions$book <- "The Trumps of Doom"

bloodamber_emotions <- blood_amber$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
bloodamber_emotions$book <- "Blood of Amber"

signchaos_emotions <- sign_chaos$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
signchaos_emotions$book <- "Sign of Chaos"

knightshadows_emotions <- knight_shadows$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
knightshadows_emotions$book <- "Knight of Shadows"

princechaos_emotions <- prince_chaos$text %>%
  get_sentences() %>%
  emotion_by(drop.unused.emotions = TRUE) %>%
  group_by(emotion_type) %>%
  summarise(ave_emotion = mean(ave_emotion)) %>%
  mutate(emotion_type = reorder(emotion_type, ave_emotion))
princechaos_emotions$book <- "Prince of Chaos"


emotions_bind <- rbind(amber_emotions, nineprinces_emotions, gunsavalon_emotions, signunicorn_emotions, handoberon_emotions, courtschaos_emotions, trumpsdoom_emotions, bloodamber_emotions, signchaos_emotions, knightshadows_emotions, princechaos_emotions)
```

```{r, warning=FALSE, echo=FALSE, fig.width=8, fig.width=10, fig.height=8}
mycolors <- c("black", "firebrick","orangered3", "salmon3", "gold4", "green4", "blue", "navy", "purple4", "darkviolet", "deeppink4", "pink4")

emotions_bind %>%
    mutate(book = fct_relevel(book, "Chronicles of Amber", "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "The Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
  ggplot(aes(reorder(emotion_type, -ave_emotion), ave_emotion, fill = book)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = mycolors) +
  ggthemes::theme_gdocs() +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10)) +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  labs(title = "Emotion Classification of Text by Book", x = "Emotion Type", y = "Average Emotion of Text",
       caption = str_wrap("Graph 3: Displaying emotion count by each line of text, returning a count for each emotion it detects (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, and any negation to those emotions)."))
```

**Graph 6:** Displaying emotion count by each line of text, returning a count for each emotion it detects (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, and any negation to those emotions). 

\newpage

```{r, echo=FALSE, message=FALSE, include=FALSE}
# Get most used words, including stop words

book_words <- chronicles_of_amber() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>%
  group_by(book) %>%
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

head(book_words)
```

```{r, echo=FALSE, include=FALSE}
# Zipf's law states that the frequency that a word appears is inversely proportional to its rank
# - rank tells us the rank of each word within the frequency table - How important is it

freq_by_rank <- book_words %>%
  group_by(book) %>%
  mutate(rank = row_number(), term_freq = n/total)

tail(freq_by_rank)

```

```{r, echo=FALSE, include=FALSE}
# Calculating the tf-idf (term frequency - inverse document frequency) attempts to find the words that are important (common) in the text, but not too common
# when idf and tf-idf are zero, these are extremely common words. This approach decreases the weight for those common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.
# tf-idf will allow us to find words that are characteristic for one book within all the books. Words that are more common in one book than another. 

book_words <- book_words %>%
  bind_tf_idf(word, book, n)

head(book_words)

# look at terms with higher tf-idf - typically proper nouns (i.e. names) 
head(book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf)))
```

\newpage

```{r, warning=FALSE, echo=FALSE, message=FALSE, fig.height=8}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = reorder_within(word, tf_idf, book)) %>%
  group_by(book) %>%
  top_n(5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, word, fill = factor(book))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Unigram TF-IDF Words from the Chronicles of Amber", x = "TF-IDF of Words", y = "Words",
       caption = "Graph 3: Displaying the top five TF-IDF words for each book.") +
  facet_wrap(~book, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = mycolors_density) +
  theme(axis.text = element_text(size = 12, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  scale_y_reordered()
```

**Graph 7:** Displaying the top five tf-idf words for each book. The term frequency - inverse document frequency (tf-idf) is the term frequency (tf) multiplied by the inverse document frequency (idf). The higher the term frequency, the lower the inverse document frequency. The tf-idf attempts to find the words that are important/common in the text, but not too common. When idf and tf-idf are zero, these are extremely common words and thus not as important. This approach decreases the weight for those common words and allows us to find words that are characteristic for one book within all the books; i.e., words that are more common in one book than another. 

\newpage


```{r, echo=FALSE, include=FALSE}
# N-Gram analysis
# bigram most frequent words that appear next to each other - token = "ngrams", n = 2
# create bigrams of the chronicles of amber
amber_grams <- amber_books %>%
  filter(chapter > 0) %>%
  unnest_tokens(bigram, text, token = "ngrams", n=2)

# view the most used bigrams
# amber_grams %>%
#   count(bigram, sort = TRUE)
snowball_sw <- get_stopwords("en", "snowball")


# adding tir na nog'th to stop words
word = c("tir", "na", "nog'th", "said")

lexicon = c( "custom",  "custom", "custom", "custom")

data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, snowball_sw)

# split the bigrams into two separate columns word1 and word2
bigrams_separated <- amber_grams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stopwords from bigrams
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word) %>%
  # filter(!word3 %in% stop_words$word) %>%
  filter(!is.na(word1)) %>%
  filter(!is.na(word2)) 
  # filter(!is.na(word3))

# new bigram counts without stopwords
bigram_counts <- bigrams_filtered %>%
  filter(chapter > 0) %>%
  count(word1, word2, sort = TRUE)

head(bigram_counts)
```


```{r, include=FALSE}
# Process tf-idf for bigrams 
bigram_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigram_united
```

```{r, echo=FALSE}
bigram_tf_idf <- bigram_united %>%
  group_by(book) %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  arrange(desc(tf_idf))
bigram_tf_idf
```

\newpage

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=10}
bigram_tf_idf %>%
  mutate(bigrams = reorder_within(bigram, tf_idf, book)) %>%
  ggplot(aes(tf_idf, bigrams, fill = factor(book))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = mycolors_density) +
  theme(axis.text = element_text(size = 12, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  labs(title = "Bigrams TF-IDF for the Chronicles of Amber", y = "Bigram", x = "tf-idf of Bigram",
       caption = "Graph 4: Displaying the top five tf-idf bigrams for each book.") +
  scale_y_reordered()
```

**Graph 8:** Displaying the top five tf-idf bigrams for each book. Bigrams identify how often one word is followed by another word. Using bigrams in conjunction with tf-idf we can find the common, but not too common bigrams for each book.

\newpage

```{r, echo=FALSE, message=FALSE, fig.height=6, include=FALSE}
left_right_bigram <- bigrams_filtered %>%
  filter(word1 == c("right", "left")) %>%
  count(word1, word2, sort = TRUE) %>%
  mutate(word2 = reorder(word2, n)) %>%
  group_by(word1) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(word2, n, fill = word1)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = c("turquoise4", "deeppink4")) +
  ggthemes::theme_gdocs() +
  labs(title = "Top 10 Bigrams for Left and Right") +
  coord_flip()

left_right_bigram

# right_bigrams <- bigrams_filtered %>%
#   filter(word1 == "right") %>%
#   count(word1, word2, sort = TRUE)
```


```{r, echo=FALSE, include=FALSE}
# Using afinn lexicon (giving a positive/negative score to each word) to graph words that appear after the word not

AFINN <- get_sentiments("afinn")
not_word <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE) %>%
  ungroup()
not_word
```


```{r, echo=FALSE, include=FALSE}
# Graph of words 

not_word %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n*value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Afinn sentiment score * number of occurences") +
  coord_flip()
```


```{r, echo=FALSE, include=FALSE}
# Network of bigrams graph - use igraph package

bigram_network_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()
# bigram_network_graph
```

```{r, fig.height=5, echo=FALSE, include=FALSE}
# graph network of bigrams - use ggraph package
# Used to show clusters of words that appear next to each other

set.seed(486)
a <- grid::arrow(type = "closed", length = unit(.07, "inches"))

ggraph(bigram_network_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05, "inches")) +
  geom_node_point(color = "navy") +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Network of Bigrams")
  theme_void()
```



```{r, echo=FALSE, include=FALSE}
# Counting and correlating pairs of words - use widyr package
# - The most often used pair of words within a 10 row section is "amber" and "time", however, time and amber are one of the most used words in the novel.

# adding tir na nog'th to stop words
word = c("tir", "na", "nog'th")

lexicon = c( "custom",  "custom", "custom")

data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, snowball_sw)

# separate the books and see which words tend to fall into the same sections - every 10 lines
amber_section_words <- chronicles_of_amber() %>%
  mutate(section = row_number() %/% 10) %>% # separate the books into sections of 10 rows
  filter(section > 0) %>% 
  unnest_tokens(word, text) %>%
  filter(!word %in% custom_stop_words$word)

# count the words co-occurring within each section - create word pairs using pairwise_count function
word_pairs <- amber_section_words %>%
  pairwise_count(word, section, sort = TRUE)


head(word_pairs %>%
  filter(item1 == "time"), 10)

head(word_pairs %>%
  filter(item1 == "amber"), 10)
```


```{r, echo=FALSE, include=FALSE}
word_cor <- amber_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)
head(word_cor)
```

\newpage

```{r, warning=FALSE, echo=FALSE, message=FALSE, fig.width=8}
mycolors2 <- c("turquoise4", "navy", "purple4", "deeppink4")

word_cor %>%
  filter(item1 %in% c("pattern", "amber", "logrus", "shadow")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(correlation, item2, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~item1, scales = "free_y") +
  scale_fill_manual(values = mycolors2) +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  labs(title = "Top 10 Correlated Words", x = "Word", y = "Correlation",
       caption = "Graph 5: Displaying the top 10 words correlated with 'Amber', 'Logrus', 'Pattern', and 'Shadow'.") +
  scale_y_reordered()
```

**Graph 9:** Displaying the top 10 words correlated with **Amber**, **Logrus**, **Pattern**, and **Shadow**.  This uses the Phi coefficient for measuring binary correlation based on how often a word appears in the same 10 line section.

\newpage

```{r, echo=FALSE, include=FALSE}
set.seed(486)

word_cor %>%
  filter(correlation > 0.21) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "violet", size = 2) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r, include=FALSE}
nine_princes_in_amber <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Nine Princes in Amber") 
nine_princes_in_amber
```

```{r, warning=FALSE, include=FALSE}
afinn <- nine_princes_in_amber %>%
  inner_join(get_sentiments("afinn"), by = join_by(word)) %>%
  group_by(index = linenumber %/% 40) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  nine_princes_in_amber %>%
    inner_join(get_sentiments("bing"), by = join_by(word)) %>%
    mutate(method = "BING"),
  nine_princes_in_amber %>%
    inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
                 filter(sentiment %in% c("positive", "negative")) %>%
    mutate(method = "NRC") 
)
  
bing_and_nrc <- bing_and_nrc %>%
    count(method, index = linenumber %/% 40, sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(sentiment = positive - negative)

bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method )) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r, echo=FALSE, include=FALSE}
# Finding the most used words based on the nrc lexicon for just the book Nine Princes in Amber

bing_word_counts <- nine_princes_in_amber %>%
  inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip()
```


```{r, echo=FALSE, include=FALSE}
# Topic modeling - per-topic-per-word probabilities, $\beta$
# Identifying top word used in each chapter of each book

amber_by_chapter <- amber_books %>%
  group_by(book) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, book, chapter)

amber_by_chapter_word <- amber_by_chapter %>%
  unnest_tokens(word, text)

word_counts <- amber_by_chapter_word %>%
  anti_join(stop_words, by = join_by(word)) %>%
  count(document, word, sort = TRUE)

head(word_counts, 10)
```


```{r, echo=FALSE, include=FALSE}
# Cast tidy data into a document text matrix
# Sparsity - sparse matrix is a matrix where most of the elements are zero. When working with text data, we say our data is "sparse" because most documents do not contain most words, resulting in a representation of mostly zeros. There are special data structures and algorithms for dealing with sparse data that can take advantage of their structure. For example, an array can more efficiently store the locations and values of only the non-zero elements instead of all elements. - Supervised Machine Learning for Text Analysis in R Emil Hvitfeldt and Julia Silge

amber_chapter_dtm <- word_counts %>%
  cast_dtm(document, word, n)
amber_chapter_dtm
```




```{r, echo=FALSE, include=FALSE}
# Latent Dirichlet Allocation (LDA) - treats each document as a mixture of topics, and each topic as a mixture of words
# Break down document term matrix into 10 different topics, iterating 500 times. With only one iteration, the topics where duplicated. For example, the word "time" was the top term for 5 out of the 10 topics.
# "Gibbs sampling is a method of Markov chain Monte Carlo (MCMC) that approximates intractable joint distribution by consecutively sampling from conditional distributions."

amber_chapter_lda <- LDA(amber_chapter_dtm, k = 10, method="Gibbs", control=list(seed = 486, iter = 500, verbose = 25))
terms(amber_chapter_lda, 5)
topics(amber_chapter_lda)
```



```{r, echo=FALSE, include=FALSE}
# Word topic probability - $\beta$ the model computes the probability of that term being generated from that topic
# For example, random has a .00000997 probability of appearing in topic 1, but a 0.01794711 probability of appearing in topic 4, etc.

amber_chapter_topic <- tidy(amber_chapter_lda, matrix = "beta")
head(amber_chapter_topic)
```



```{r, echo=FALSE, include=FALSE}
# Top five terms probability per topic - ganelon has the highest probability, 0.029, of showing up in topic 1
# showing the top 5 terms prob per topic

amber_top_terms <- amber_chapter_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup() %>%
  arrange(topic, -beta)
head(amber_top_terms, 10)
mean(amber_top_terms$beta)
```

\newpage

```{r, echo=FALSE, fig.height=8, fig.width=12}
# Visual representation of top five word probability of showing up with each topic. 

amber_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = mycolors_density) +
  labs(title = "Top 5 Word Probability (Beta) Per Topic", x = "Beta Probability", y = "Term",
       caption = "Graph 6: Visual representation of top five word with the highest beta probability of showing up in each topic.") +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  scale_y_reordered()
```

**Graph 10:** Visual representation of top five word with the highest beta probability of showing up in each topic.

\newpage

```{r, echo=FALSE, fig.height=8, fig.width=12}
# Visualizing - for topic 1, any gamma greater than 0.2. We can see that approximately 100% of the words from Sign of Chaos Chapter 8, 10, 11, and 12 are generated from topic 1. However, we can also see that 100% of the words in Knight of Shadows chapter 6, 7, and 12 are also generated from topic 1. 

amber_chapter_gamma <- tidy(amber_chapter_lda, matrix = "gamma")
# head(amber_chapter_gamma)

amber_gamma_graph <- amber_chapter_gamma %>%
  group_by(topic) %>%
  slice_max(gamma, n = 5) %>%
  ungroup() %>%
  arrange(topic, -gamma)
# head(amber_top_terms, 10)

amber_gamma_graph %>%
  mutate(document = reorder_within(document, gamma, topic)) %>%
  ggplot(aes(gamma, document, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = mycolors_density) +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  labs(title = "Top 5 Document Probability (Gamma) per Topic", x = "Gamma Probability", y = "Document",
       caption = "Graph 7: Visual representation of top five documents with the highest gamma probability of showing up in each topic." ) +
  scale_y_reordered()

# median(amber_chapter_gamma$gamma)
```


```{r, echo=FALSE, include=FALSE}
# Split book and chapter to view per-document-per-topic probability

amber_chapter_gamma <- amber_chapter_gamma %>%
  separate(document, c("title","chapter"), sep = "_", convert = TRUE)
head(amber_chapter_gamma)

```

\newpage

```{r, echo=FALSE, fig.width=12, fig.height=8}
# Each chapter is supposed to be uniquely assigned to a specific topic.

amber_chapter_gamma %>%
  mutate(title = fct_relevel(title, "Nine Princes in Amber", "The Guns of Avalon", "Sign of the Unicorn", "The Hand of Oberon", "The Courts of Chaos", "The Trumps of Doom", "Blood of Amber", "Sign of Chaos", "Knight of Shadows", "Prince of Chaos")) %>%
  ggplot(aes(factor(topic), gamma, fill = title)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~title, ncol = 2) +
  scale_fill_manual(values = mycolors_density) +
  theme(axis.text = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
        plot.caption = element_text(hjust = 1, size = 10),
        strip.text = element_text(face = "bold", size = 12)) +
  labs(title = "Gamma Probability Assignment by Book/Topic" , x = "topic", y = expression(gamma),
       caption = "Graph 8: Displaying Topic modeling - per-document-per-topic probabilities, gamma.")
```

**Graph 11:** Displaying Topic modeling - per-document-per-topic probabilities, $\gamma$. Where $\gamma$ is an estimated proportion of words from that document that are generated from that topic. Displaying the probability of each chapter within each book.

\newpage

```{r, echo=FALSE, include=FALSE}
# Classification of the topics - topic 1 has a gamma probability of approximately 25% chance of appearing in The Guns of Avalon chapter 1
  
chapter_classification <- amber_chapter_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()
head(chapter_classification, 10)
```



```{r, echo=FALSE, warning=FALSE, include=FALSE}
# 284 rows out of 350 rows or 81% of the topics were misclassified. This makes sense because of the series has the same theme/topics throughout the series and a lot of the same characters.

book_topics <- chapter_classification %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

chpt_consensus <- chapter_classification %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)

head(chpt_consensus)


```



```{r, warning=FALSE, echo=FALSE, include=FALSE}
# augmenting - assigning each word in each document to a topic - the more words in a document assigned to that topic, generally, the more weight gamma will go on that document-topic classification 
# returns a tidy data frame of book-term counts
# Because the LDA algorithm is stochastic, it can land on a topic that spans multiple books (stochastic = randomly determined; having random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely)

assignments <- augment(amber_chapter_lda, data = amber_chapter_dtm)
head(assignments, 10)
```

```{r, warning=FALSE, echo=FALSE, include=FALSE}
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))
head(assignments)
```


```{r, warning=FALSE, echo=FALSE}
assignments %>%
  count(title, consensus, wt=count) %>%
  mutate(across(c(title, consensus), ~str_wrap(., 20))) %>%
  group_by(title) %>%
  mutate(percent = n/sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "navy", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignment")
```





```{r, warning=FALSE, echo=FALSE, include=FALSE}
# what were the most commonly mistaken words

wrong_words <- assignments %>%
  filter(title != consensus)

wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```






























