---
title: "Chronicles_of_Amber"
author: "Crystal Sawtelle"
date: "2023-03-03"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(Amber)
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(dplyr)
library(stringr)
install.packages("tidytext", repos = "http://cran.us.r-project.org")
library(tidytext)
# need for afinn lexicon
install.packages("textdata", repos = "http://cran.us.r-project.org")
library(textdata)
install.packages("textclean", repos = "http://cran.us.r-project.org")
library(textclean)
install.packages("tm", repos = "http://cran.us.r-project.org")
library(tm)
# used for word cloud
install.packages("wordcloud", repos = "http://cran.us.r-project.org")
library(wordcloud)
# reshape2 is used to make a comparison cloud
install.packages("reshape2", repos = "http://cran.us.r-project.org")
library(reshape2)
# used to manipulate and analyze networds - create columns of "from" and "to" to visualize a network
install.packages("igraph", repos = "http://cran.us.r-project.org")
library(igraph)
# used to graph network created from igraph package
install.packages("ggraph", repos = "http://cran.us.r-project.org")
library(ggraph)
# used for counting and correlating pairs of words within sections of text
install.packages("widyr", repos = "http://cran.us.r-project.org")
library(widyr)
library(tidyr)
library(ggplot2)
```


```{r, echo=FALSE}
amber_books <- chronicles_of_amber() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]")))) %>%
  ungroup()

#amber_books
```




```{r, echo=FALSE}
# using tidytext to unnest each word in the book

amber_tidy <- amber_books %>%
  unnest_tokens(word, text)
#amber_tidy
```

```{r, echo=FALSE}
# remove stop words "the", "a", "if", etc.

data("stop_words")

amber_tidy <- amber_tidy %>%
  anti_join(stop_words, by = "word")

amber_tidy %>%
  count(word, sort = TRUE)

```



Will need to add a few word contractions like "i'd", "i'm", and "it's" to the stop_words. There is a difference between the  and the one used in stop_words.


```{r, echo=FALSE}
# graph the top most used words

amber_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()


```

```{r, echo=FALSE}
# create a custom stop words list to add to standard stop words
word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve", "i’ve", "hadn’t", "we’ve", "i’ll", "he’s", "can’t", "wasn’t", "what’s", "won’t", "we’d", "doesn’t", "you’d", "he’d", "wouldn’t", "let’s", "there’s", "she’s", "she’d", "you’ll", "they’re", "we’ll", "weren’t", "aren’t")

lexicon = c( "custom",  "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")

data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, stop_words)
# custom_stop_words

# rerunning amber tidy with new custom_stop_words
amber_tidy <- anti_join(amber_tidy, custom_stop_words, by = "word")

amber_tidy %>%
  count(word, sort = TRUE)

amber_tidy_graph <- amber_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "navy") +
  coord_flip()

amber_tidy_graph %>% plotly::ggplotly()
```






Start individual book word frequency analysis
```{r, echo=FALSE}
nine_princes_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Nine Princes in Amber") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

nine_princes_graph <- ggplot(nine_princes_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "Nine Princes of Amber") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# nine_princes_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
guns_avalon_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Guns of Avalon") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

guns_avalon_graph <- ggplot(guns_avalon_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "The Guns of Avalon") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# guns_avalon_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
sign_unicorn_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Sign of the Unicorn") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

sign_unicorn_graph <- ggplot(sign_unicorn_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "Sign of the Unicorn") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# sign_unicorn_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
hand_oberon_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Hand of Oberon") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

hand_oberon_graph <- ggplot(hand_oberon_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "The Hand of Oberon") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# hand_oberon_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
courts_chaos_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Courts of Chaos") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

courts_chaos_graph <- ggplot(courts_chaos_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "The Courts of Chaos") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# courts_chaos_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
trumps_doom_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Trumps of Doom") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

trumps_doom_graph <- ggplot(trumps_doom_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "The Trumps of Doom") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# trumps_doom_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
blood_amber_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Blood of Amber") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

blood_amber_graph <- ggplot(blood_amber_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "Blood of Amber") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# blood_amber_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
sign_chaos_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Sign of Chaos") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

sign_chaos_graph <- ggplot(sign_chaos_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "Sign of Chaos") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# sign_chaos_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
knight_shadow_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Knight of Shadows") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

knight_shadow_graph <- ggplot(knight_shadow_word, aes(word, n)) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) + 
  labs(title = "Knight of Shadows") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# knight_shadow_graph %>% plotly::ggplotly()

```

```{r, echo=FALSE}
prince_chaos_word <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Prince of Chaos") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  slice(1:5)

prince_chaos_graph <- ggplot(prince_chaos_word, aes(word, n), fill = word) +
  geom_col(fill = c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1", "lightsteelblue")) +
  coord_flip(ylim = c(0, 220)) +
  labs(title = "Prince of Chaos") +
  theme(plot.title = element_text(size = 14), axis.text = element_text(size = 14))

# prince_chaos_graph %>% plotly::ggplotly()

```

```{r, fig.height=8, echo=FALSE}
cowplot::plot_grid(nine_princes_graph, guns_avalon_graph, sign_unicorn_graph, hand_oberon_graph, courts_chaos_graph, 
                   trumps_doom_graph, blood_amber_graph, sign_chaos_graph, knight_shadow_graph, prince_chaos_graph, ncol = 2)
```























```{r, warning=FALSE, echo=FALSE}
# graph the sentiment of every 40 pages
amber_sentiment <- amber_tidy %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(book, index = linenumber %/% 40, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
Using low-pass Fourier transform
```{r}

```


### Figure: Words that contribute to positive and negative sentiment in Roger Zelazny's novel Nine Princes of Amber

```{r}
amber_tidy %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Identify the top 10 positive and top 10 negative words in the book series.
Will remove words that are names because they do not have a sentiment of positive or negative (i.e. corwin, random, amber, and luke)
```{r, warning=FALSE}
amber_bing_word_counts <- amber_tidy %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

amber_bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip()
```

Create a word cloud separated by positive and negative words.
```{r, fig.height=3, warning=FALSE}
amber_tidy %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("steelblue4", "magenta4"), max.words = 100)
  
```

Unnest by sentences
```{r}
amber_sentences <- chronicles_of_amber() %>%
  unnest_tokens(sentence, text, token = "sentences")

amber_sentences$sentence[20]
```

Split by chapter and find the most negative and positive chapters
```{r}
amber_chapters <- chronicles_of_amber() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|chapter [\\divxlc]") %>%
  ungroup()

amber_chapters %>%
  group_by(book) %>%
  summarise(chapters = n())
```

Most negative chapters
```{r, warning=FALSE}
amber_negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

amber_word_count <- amber_tidy %>%
  group_by(book, chapter) %>%
  summarise(words = n())

amber_tidy %>%
  semi_join(amber_negative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

Get most positive chapters
```{r, warning=FALSE}
amber_positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

amber_tidy %>%
  semi_join(amber_positive) %>%
  group_by(book, chapter) %>%
  summarize(positive_words = n()) %>%
  left_join(amber_word_count, by = c("book", "chapter")) %>%
  mutate(ratio = positive_words/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

Get most used words, including stop words
```{r}
book_words <- chronicles_of_amber() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>%
  group_by(book) %>%
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

Find the term frequency distribution of the book words for each novel
```{r}
tf_dist_amber <- ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
tf_dist_amber
```
Zipf's law states that the frequency that a word appears is inversely proportional to its rank
- rank tells us the rank of each word within the frequency table - How important is it
```{r}
freq_by_rank <- book_words %>%
  group_by(book) %>%
  mutate(rank = row_number(), term_freq = n/total)

head(freq_by_rank)
tail(freq_by_rank)

```

plot the x-axis and term frequency on the y-axis on a logarithmic scale 
the inversely proportional relationship will have a constant negative slope
```{r}
freq_by_rank %>%
  ggplot(aes(rank, term_freq, color = book)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

```{r}
rank_subset <- freq_by_rank %>%
  filter(rank < 500, rank > 10)

lm(log10(term_freq) ~ log10(rank), data = rank_subset)

freq_by_rank %>%
  ggplot(aes(rank, term_freq, color = book)) +
  geom_abline(intercept = -0.77, slope = -1.04, color = "navy", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

Calculating the tf-idf (term frequency - inverse document frequency) attempts to find the words that are important (common) in the text, but not too common
when idf and tf-idf are zero, these are extremely common words. This approach decreases the weight for those common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.
tf-idf will allow us to find words that are characteristic for one book within all the books. Words that are more common in one book than another. 
```{r}
book_words <- book_words %>%
  bind_tf_idf(word, book, n)

book_words

# look at terms with higher tf-idf - typically proper nouns (i.e. names) 
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r, warning=FALSE}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(book) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(word, tf_idf), tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
```

N-Gram analysis
We are going to start with two words using token = "ngrams", n = 2
```{r}
# create bigrams of the chronicles of amber
amber_grams <- chronicles_of_amber() %>%
  unnest_tokens(bigram, text, token = "ngrams", n=2)

# view the most used bigrams
# amber_grams %>%
#   count(bigram, sort = TRUE)

# split the bigrams into two separate columns word1 and word2
bigrams_separated <- amber_grams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stopwords from bigrams
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word) %>%
  filter(!is.na(word1)) %>%
  filter(!is.na(word2))

# new bigram counts without stopwords
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts
```

Process tf-idf for bigrams 
```{r}
bigram_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigram_united
```

```{r}
bigram_tf_idf <- bigram_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf
```

```{r, warning=FALSE}
bigram_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigrams = factor(bigram, levels = rev(unique(bigram)))) %>%
  group_by(book) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(bigram, tf_idf), tf_idf, fill = book)) +
  geom_col(stat = "idendity", show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf of bigram to novel") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
```

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

```{r}
AFINN <- get_sentiments("afinn")
not_word <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE) %>%
  ungroup()
not_word
```

```{r}
not_word %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n*value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Afinn sentiment score * number of occurences") +
  coord_flip()
```

Network of bigrams graph - use igraph package
```{r}
bigram_network_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()
bigram_network_graph
```

graph network of bigrams - use ggraph package
```{r}
set.seed(486)
a <- grid::arrow(type = "closed", length = unit(.07, "inches"))

ggraph(bigram_network_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05, "inches")) +
  geom_node_point(color = "lightblue", size = 2) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Counting and correlating pairs of words - use widyr package
- The most often used pair of words within a 10 row section is "amber" and "time", however, time and amber are one of the most used words in the novel.
```{r}
# separate the books and see which words tend to fall into the same sections - every 10 lines
amber_section_words <- chronicles_of_amber() %>%
  mutate(section = row_number() %/% 10) %>% # separate the books into sections of 10 rows
  filter(section > 0) %>% 
  unnest_tokens(word, text) %>%
  filter(!word %in% custom_stop_words$word)

# count the words co-occurring within each section - create word pairs using pairwise_count function
word_pairs <- amber_section_words %>%
  pairwise_count(word, section, sort = TRUE)


word_pairs %>%
  filter(item1 == "time")

word_pairs %>%
  filter(item1 == "amber")
```

Find the correlation among words indicating how often they appear together relative to how often they appear separately. 
This uses the Phi coefficient (equivalent to the Pearson correlation) for measuring binary correlation. Focusing on how much more likely either both word X and Y appear together or neither appear together, or they appear without each other. 

$n_{11}$ = Has word X and word Y
$n_{10}$ = Has word X but not word Y
$n_{01}$ = Not word X but has word Y
$n_{00}$ = Not word X or word Y
$n_{1.}$ = Row total of Has word X
$n_{0.}$ = Row total of no word X
$n_{.1}$ = Column total has word Y
$n_{.0}$ = Column total no word Y
n = total

$$ \phi = \frac{n_{11}n_{00} - n_{10}n_{01}}{\sqrt{n_{1.}n_{0.}n_{.0}n_{.1}}}$$
pairwise_cor() function finds the phi coefficients between words based on how often the appear in the same section
```{r}
word_cor <- amber_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)
word_cor
```

```{r}
word_cor %>%
  filter(item1 %in% c("pattern", "amber", "logrus", "shadow")) %>%
  group_by(item1) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~item1, scales = "free") +
  coord_flip()
```

```{r}
set.seed(486)

word_cor %>%
  filter(correlation > 0.20) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "violet", size = 2) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```



















Individual book analysis
```{r}
nine_princes_in_amber <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Nine Princes in Amber") 
# nine_princes_in_amber
```

```{r, warning=FALSE}
afinn <- nine_princes_in_amber %>%
  inner_join(get_sentiments("afinn"), by = join_by(word)) %>%
  group_by(index = linenumber %/% 40) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  nine_princes_in_amber %>%
    inner_join(get_sentiments("bing"), by = join_by(word)) %>%
    mutate(method = "BING"),
  nine_princes_in_amber %>%
    inner_join(get_sentiments("nrc"), by = join_by(word)) %>%
                 filter(sentiment %in% c("positive", "negative")) %>%
    mutate(method = "NRC") 
)
  
bing_and_nrc <- bing_and_nrc %>%
    count(method, index = linenumber %/% 40, sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(sentiment = positive - negative)

bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method )) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r}
bing_word_counts <- nine_princes_in_amber %>%
  inner_join(get_sentiments("bing"), by = join_by(word)) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip()
```

```{r}
the_guns_of_avalon <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Guns of Avalon")
# the_guns_of_avalon
```

```{r}
sign_of_the_unicorn <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Sign of the Unicorn")
# sign_of_the_unicorn
```

```{r}
the_hand_of_oberon <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Hand of Oberon")
# the_hand_of_oberon
```

```{r}
the_courts_of_chaos <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Courts of Chaos")
# the_courts_of_chaos
```

```{r}
the_trumps_of_doom <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "The Trumps of Doom")
# the_trumps_of_doom
```

```{r}
blood_of_amber <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Blood of Amber")
# blood_of_amber
```

```{r}
sign_of_chaos <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Sign of Chaos")
# sign_of_chaos
```

```{r}
knight_of_shadows <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Knight of Shadows")
# knight_of_shadows
```

```{r}
prince_of_chaos <- amber_tidy %>%
  group_by(book) %>%
  filter(book == "Prince of Chaos")
# prince_of_chaos
```





























```{r}
# break out the first book in the series
nine_princes_in_amber <- chronicles_of_amber() %>%
  group_by(book) %>%
  filter(book == "Nine Princes in Amber") %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]"))))

# nine_princes_in_amber
```

