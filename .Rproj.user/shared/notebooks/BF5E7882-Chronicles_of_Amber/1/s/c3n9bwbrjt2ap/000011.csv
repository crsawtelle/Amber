"0","# create a custom stop words list to add to standard stop words - removing mostly names of characters"
"0","word = c(""random"", ""luke"", ""ganelon"", ""brand"", ""benedict"", ""jurt"", ""julian"", ""dalt"", ""bill"", ""left"", ""eric"", ""bleys"", ""jasra"", ""dalt's"", ""dalt"", ""mandor"", ""caine"", ""gerard"", ""nayda"", ""amber"", ""time"")"
"0",""
"0","lexicon = c( ""custom"",  ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"", ""custom"",  ""custom"",  ""custom"",  ""custom"",  ""custom"",  ""custom"",  ""custom"", ""custom"", ""custom"")"
"0",""
"0","data <- data.frame(word, lexicon)"
"0","custom_stop_words <- rbind(data, stop_words)"
"0",""
"0","amber_by_chapter <- amber_books %>%"
"0","  group_by(book) %>%"
"0","  mutate(chapter = cumsum(str_detect(text, regex(""^chapter "", ignore_case = TRUE)))) %>%"
"0","  ungroup() %>%"
"0","  filter(chapter > 0) %>%"
"0","  unite(document, book, chapter)"
"0",""
"0","amber_by_chapter_word <- amber_by_chapter %>%"
"0","  unnest_tokens(word, text)"
"0",""
"0","word_counts <- amber_by_chapter_word %>%"
"0","  anti_join(stop_words) %>%"
"0","  count(document, word, sort = TRUE)"
"2","Joining with `by = join_by(word)`"
"0","install.packages(""textclean"", repos = ""http://cran.us.r-project.org"")"
"1","Warning in "
"1",""
"1","install.packages"
"1",""
"1"," :
  "
"1",""
"1","package ‘textclean’ is in use and will not be installed"
"1",""
"1","
"
"0","word_counts"
