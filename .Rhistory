amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# create a custom stop words list to add to standard stop words
custom_stop_words <- bind_rows(data_frame(word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve",
# create a custom stop words list to add to standard stop words
custom_stop_words <- bind_rows(data_frame(word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve", "i’ve", "hadn’t", "we’ve", "i’ll", "he’s", "can’t", "wasn’t", "what’s", "won’t", "we’d", "doesn’t", "you’d", "he’d", "wouldn’t", "let’s", "there’s", "she’s", "she’d", "you’ll", "they’re", "we’ll", "weren’t", "aren’t"),
library(Amber)
?chronicles_of_amber
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
amber_tidy <- amber_tidy %>%
anti_join(stop_words, by = join_by("word")) %>%
count(word, sort = TRUE)
amber_tidy
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = join_by("word"))
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = join_by("word"))
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = "word")
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
View(amber_tidy)
amber_tidy
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n))
amber_tidy
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
ggplot(amber_tidy, aes(word, n)) +
geom_col() +
coord_flip()
# graph the top most used words
amber_tidy_graph %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n))
# graph the top most used words
amber_tidy_graph <- amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n))
ggplot(amber_tidy_graph, aes(word, n)) +
geom_col() +
coord_flip()
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(amber_tidy, aes(word, n)) +
geom_col() +
coord_flip()
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
amber_tidy_graph <- amber_tidy %>%
filter(n > 600) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
knitr::opts_chunk$set(echo = TRUE)
library(Amber)
install.packages("tidyverse")
library(tidyverse)
library(dplyr)
library(stringr)
install.packages("tidytext")
library(tidytext)
install.packages("textclean")
library(textclean)
install.packages("tm")
library(tm)
library(tidyr)
amber_books <- chronicles_of_amber() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]")))) %>%
ungroup()
amber_books
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = "word")
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
install.packages("textclean")
install.packages("tm")
install.packages("tidyverse")
install.packages("tidytext")
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = "word")
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# graph the top most used words
amber_tidy_graph <- amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
amber_tidy_graph
View(amber_tidy_graph)
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- anti_join(amber_tidy, stop_words, by = "word")
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
# graph the top most used words
amber_tidy_graph <- amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
amber_tidy_graph
ggplot(amber_tidy, aes(word, n)) +
geom_col() +
coord_flip()
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# amber_tidy_graph
#
# ggplot(amber_tidy, aes(word, n)) +
#   geom_col() +
#   coord_flip()
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
amber_tidy <- amber_tidy %>%
anti_join(stop_words, by = "word")
amber_tidy <- amber_tidy %>%
count(word, sort = TRUE)
amber_tidy
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
amber_tidy %>%
count(word, sort = TRUE)
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
amber_tidy <- amber_tidy %>%
anti_join(stop_words, by = "word")
amber_tidy %>%
count(word, sort = TRUE)
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# create a custom stop words list to add to standard stop words
custom_stop_words <- bind_rows(data_frame(word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve", "i’ve", "hadn’t", "we’ve", "i’ll", "he’s", "can’t", "wasn’t", "what’s", "won’t", "we’d", "doesn’t", "you’d", "he’d", "wouldn’t", "let’s", "there’s", "she’s", "she’d", "you’ll", "they’re", "we’ll", "weren’t", "aren’t"),
knitr::opts_chunk$set(echo = TRUE)
library(Amber)
install.packages("tidyverse")
library(tidyverse)
library(dplyr)
library(stringr)
install.packages("tidytext")
library(tidytext)
install.packages("textclean")
library(textclean)
install.packages("tm")
library(tm)
library(tidyr)
amber_books <- chronicles_of_amber() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]")))) %>%
ungroup()
amber_books
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- amber_tidy %>%
anti_join(stop_words, by = "word")
amber_tidy %>%
count(word, sort = TRUE)
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# amber_tidy_graph
#
# ggplot(amber_tidy, aes(word, n)) +
#   geom_col() +
#   coord_flip()
# create a custom stop words list to add to standard stop words
word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve", "i’ve", "hadn’t", "we’ve", "i’ll", "he’s", "can’t", "wasn’t", "what’s", "won’t", "we’d", "doesn’t", "you’d", "he’d", "wouldn’t", "let’s", "there’s", "she’s", "she’d", "you’ll", "they’re", "we’ll", "weren’t", "aren’t")
lexicon = c( "custom",  "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, stop_words)
custom_stop_words
# rerunning amber tidy with new custom_stop_words
amber_tidy <- anti_join(amber_tidy, custom_stop_words, by = "word")
amber_tidy %>%
count(word, sort = TRUE)
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# graph the sentiment of every 20 pages
amber_sentiment <- amber_tidy %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 20, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
install.packages("plyr", repos = "http://cran.us.r-project.org")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidytext")
install.packages("tidytext")
install.packages("tm")
install.packages("tm")
knitr::opts_chunk$set(echo = TRUE)
library(Amber)
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(dplyr)
library(stringr)
install.packages("tidytext", repos = "http://cran.us.r-project.org")
library(tidytext)
install.packages("textclean", repos = "http://cran.us.r-project.org")
library(textclean)
install.packages("tm", repos = "http://cran.us.r-project.org")
library(tm)
library(tidyr)
amber_books <- chronicles_of_amber() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]")))) %>%
ungroup()
#amber_books
# using tidytext to unnest each word in the book
amber_tidy <- amber_books %>%
unnest_tokens(word, text)
#amber_tidy
# remove stop words "the", "a", "if", etc.
data("stop_words")
amber_tidy <- amber_tidy %>%
anti_join(stop_words, by = "word")
amber_tidy %>%
count(word, sort = TRUE)
# graph the top most used words
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# amber_tidy_graph
#
# ggplot(amber_tidy, aes(word, n)) +
#   geom_col() +
#   coord_flip()
# create a custom stop words list to add to standard stop words
word = c("i’d", "i’m", "it’s", "don’t", "we’ve", "have’t", "didn’t", "you’re", "you’ve", "i’ve", "hadn’t", "we’ve", "i’ll", "he’s", "can’t", "wasn’t", "what’s", "won’t", "we’d", "doesn’t", "you’d", "he’d", "wouldn’t", "let’s", "there’s", "she’s", "she’d", "you’ll", "they’re", "we’ll", "weren’t", "aren’t")
lexicon = c( "custom",  "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
data <- data.frame(word, lexicon)
custom_stop_words <- rbind(data, stop_words)
custom_stop_words
# rerunning amber tidy with new custom_stop_words
amber_tidy <- anti_join(amber_tidy, custom_stop_words, by = "word")
amber_tidy %>%
count(word, sort = TRUE)
amber_tidy %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip()
# graph the sentiment of every 20 pages
amber_sentiment <- amber_tidy %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# graph the sentiment of every 20 pages
amber_sentiment <- amber_tidy %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 40, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# graph the sentiment of every 20 pages
amber_sentiment <- amber_tidy %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 40, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(amber_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
knitr::opts_chunk$set(echo = TRUE)
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(rvest)) install.packages('rvest')
library(ggplot2)
url <- 'https://www.imdb.com/search/title/?count=100&release_date=2016,2016&title_type=feature'
page <- read_html(url)
page
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, '.text-primary')
# convert the ranking data to text
rank_data <- as.numeric(html_text(rank_data_html))
head(rank_data)
length(rank_data)
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(rvest)) install.packages('rvest')
library(ggplot2)
url <- 'https://www.census.gov/popclock/world/ag'
page <- read_html(url)
page
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, 'basic-facts > div:nth-child(2) > h2')
# convert the ranking data to text
rank_data <- as.numeric(html_text(rank_data_html))
head(rank_data)
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, 'basic-facts > div:nth-child(2) > h2')
# convert the ranking data to text
rank_data <- as.numeric(html_attr(rank_data_html))
head(rank_data)
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, 'h2')
# convert the ranking data to text
rank_data <- as.numeric(html_attr(rank_data_html))
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, '<h2 data-population="">44.8M</h2>')
# convert the ranking data to text
rank_data <- as.numeric(html_attr(rank_data_html))
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, 'h2 data-population=""/h2')
# Using CSS selectors to scrape the ranking section
rank_data_html <- html_nodes(page, '#basic-facts > div:nth-child(2) > h2')
# convert the ranking data to text
rank_data <- as.numeric(html_attr(rank_data_html))
# convert the ranking data to text
rank_data <- as.numeric(html_text(rank_data_html))
head(rank_data)
url <- 'https://www.census.gov/popclock/world/ag'
page <- read_html(url)
page
summaries_xpath <- census %>%
html_elements(css = ".data-cell")
summaries_xpath <- page %>%
html_elements(css = ".data-cell")
head(summaries_xpath)
View(summaries_xpath)
census_summaries_css <- html_text(summaries_xpath)
census_summaries_css
View(census_summaries_css)
summaries_xpath <- page %>%
html_elements(css = "data-cell")
View(summaries_xpath)
census_summaries_css <- html_text(summaries_xpath)
View(census_summaries_css)
summaries_xpath <- page %>%
html_elements(css = "<h2 data-population="">44.8M</h2>")
summaries_xpath <- page %>%
html_elements(css = "#basic-facts > div:nth-child(2) > h2")
View(summaries_xpath)
census_summaries_css <- html_text(summaries_xpath)
census_summaries_css
summaries_xpath <- page %>%
html_elements(css = "h2")
View(summaries_xpath)
census_summaries_css <- html_text(summaries_xpath)
census_summaries_css
summaries_xpath <- page %>%
html_elements(css = "//*[@id="basic-facts"]/div[1]/h2")
summaries_xpath <- page %>%
html_elements(css = "//*[@id="basic-facts"], h2")
summaries_xpath <- page %>%
html_elements(xpath = "//*[@id="basic-facts"]/div[1]/h2")
summaries_xpath <- page %>%
html_elements(xpath = "//*[contains(@id, "basic-facts")]")
summaries_xpath <- page %>%
html_elements(xpath = "//*[contains(@id, 'basic-facts')]")
summaries_xpath
census_summaries_css <- html_text(summaries_xpath)
census_summaries_css
summaries_xpath <- page %>%
html_elements(xpath = "//*[@id="basic-facts"]/div[1]")
?html_element
url <- 'https://www.census.gov/popclock/world/ag'
page <- read_html(url)
summaries_xpath <- page %>%
html_elements(xpath = "//*[@id="basic-facts"]/div[1]/h2")
url <- 'https://www.census.gov/popclock/world/ag'
summaries_xpath <- page %>%
html_elements(xpath = "//*[@id="basic-facts"]/div[1]/h2")
summaries_xpath <- page %>%
html_elements(css = basic-facts > div:nth-child(2) > h2)
summaries_xpath <- page %>%
html_elements(xpath = "/html/body/div[2]/div[3]/div[1]/div[1]/h2")
summaries_xpath
census_summaries_css <- html_text(summaries_xpath)
census_summaries_css
